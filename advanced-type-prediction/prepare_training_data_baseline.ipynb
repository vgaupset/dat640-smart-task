{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parepare training data for baseline features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys,json,re,string\n",
    "from typing import Callable, Dict, List, Set, Tuple\n",
    "from elasticsearch import Elasticsearch\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import datetime\n",
    "import csv\n",
    "sys.path.insert(1, 'extract_features')\n",
    "sys.path.insert(1, 'util')\n",
    "from helper_function import preprocess\n",
    "from extract_No1to5_features_optimized import extract_features_1to5\n",
    "from extract_No11to12_features_optimized import extract_features_11to12, get_collection_paramter\n",
    "from extract_No13to15_features import TypeTaxonomy, extract_features_13to15\n",
    "from extract_No16_feature import extract_features_16\n",
    "from extract_No17to19_features import get_analyze,extract_features_17to19\n",
    "from extract_No20to22_features import extract_features_20to22\n",
    "from extract_No23to25_features_optimized import extract_features_23to25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'LAPTOP-ADBLIUPR',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': '5VELmBwJTk-urTuhZdTgew',\n",
       " 'version': {'number': '7.15.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': '83c34f456ae29d60e94d886e455e6a3409bba9ed',\n",
       "  'build_date': '2021-10-07T21:56:19.031608185Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.9.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce logging level.\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "logging.getLogger(\"elasticsearch\").disabled = True\n",
    "\n",
    "es= Elasticsearch(timeout=600)\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/junec/anaconda3/python\n",
      "/c/Users/junec/anaconda3/Scripts/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files\n",
    "filepath=\"data/DBpedia_map_type_entities.json\"\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    DBpedia_map_type_entities = json.load(file)\n",
    "    \n",
    "filepath=\"data/training_types.json\"\n",
    "with open(filepath,encoding='utf-8') as json_file:\n",
    "    training_map_type_questions = json.load(json_file)\n",
    "\n",
    "filepath=\"data/ElasticSearch_map_type_docID.json\"\n",
    "with open(filepath, 'r',encoding='utf-8') as f:\n",
    "    docID_DBOtype_dict = json.load(f)\n",
    "    \n",
    "filepath=\"data/DBpedia_map_docID_docLength.json\"\n",
    "with open(filepath, 'r',encoding='utf-8') as f:\n",
    "     map_docID_docLength = json.load(f)\n",
    "\n",
    "filepath=\"../smart-dataset/datasets/DBpedia/smarttask_dbpedia_train.json\"\n",
    "with open(filepath, 'r') as f:\n",
    "    smarttask_dbpedia_train = json.load(f)\n",
    "    \n",
    "\n",
    "typeobj=TypeTaxonomy(\"data/dbpedia_types.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_baseline(\n",
    "    question:str,\n",
    "    dp_type: str,\n",
    "    DBpedia_map_type_entities:Dict,\n",
    "    docID_DBOtype_dict:Dict,\n",
    "    map_docID_docLength:Dict,\n",
    "    collection_len:int,doc_number:int,avgdl:float,\n",
    "    es: Elasticsearch,\n",
    " \n",
    ") -> List[float]:\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Args:\n",
    "            query: string.\n",
    "            dp_type: DBO type.\n",
    "            DBpedia_map_type_entities:DBOtype and all the DBpedia entities having that type.\n",
    "            docID_DBOtype_dict:dictionary for elasticseach doc_id and DBOtype.\n",
    "            es: Elasticsearch object instance.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    features_1to5 = extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "    feature_vect = list(features_1to5.values())\n",
    "\n",
    "    features_11to12 = extract_features_11to12(dp_type, question,\n",
    "                                              docID_DBOtype_dict,\n",
    "                                              map_docID_docLength,\n",
    "                                              collection_len,doc_number,avgdl,\n",
    "                                              es)\n",
    "    feature_vect.extend(list(features_11to12.values()))\n",
    "    \n",
    "    return feature_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with first path filter\n",
    "def prepare_training_data_baseline( smarttask_dbpedia_train:List[Dict],\n",
    "                            DBpedia_map_type_entities:Dict[str,List],\n",
    "                            docID_DBOtype_dict:Dict[str,str],\n",
    "                            map_docID_docLength:Dict[str,int], \n",
    "                            es: Elasticsearch\n",
    "                            )-> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"X and y label for base line features. Save to csv file.\n",
    "         Args:\n",
    "            smarttask_dbpedia_train: smarttask training set.\n",
    "            DBpedia_map_type_entities:DBOtype and all the DBpedia entities having that type.\n",
    "            docID_DBOtype_dict:dictionary for elasticseach doc_id and DBOtype.\n",
    "            es: Elasticsearch object instance.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    collection_len,doc_number,avgdl=get_collection_paramter(es,field=\"abstract\",index=\"dbpdiea_type_centric\")\n",
    "    \n",
    "    \n",
    "    count=0\n",
    "    j=0\n",
    "    print(\"-----------------\",datetime.datetime.now())\n",
    "    with open(\"data/for_training_baseline.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "\n",
    "        for entry in smarttask_dbpedia_train:\n",
    " \n",
    "            if entry['question']==None:\n",
    "                continue\n",
    "\n",
    "            question_processed=preprocess(entry['question'])\n",
    "\n",
    "            if entry['category']=='resource':\n",
    "                if count%30==0:\n",
    "                    print(\"--------------------count:\",count)\n",
    "                count+=1\n",
    "                #print(\"question_processed:\",question_processed)\n",
    "                for DBOtype in entry['type']: \n",
    "                    try:\n",
    "                        features=extract_features_baseline(question_processed,\n",
    "                                                            DBOtype,\n",
    "                                                            DBpedia_map_type_entities,\n",
    "                                                            docID_DBOtype_dict,\n",
    "                                                            map_docID_docLength,                                                \n",
    "                                                            collection_len,doc_number,avgdl,\n",
    "                                                            es)\n",
    "                    except BaseException as err:\n",
    "                        print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                        print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                        raise\n",
    "                    j+=1\n",
    "                    if j%500==0:\n",
    "                        print(\"-----------------\",datetime.datetime.now())\n",
    "                        print(\"--------------------j:\",j)\n",
    "                    writer.writerow(features)\n",
    "                    writer.writerow([1])\n",
    "\n",
    "\n",
    "                #deal with top 30 documents\n",
    "                hits = es.search(\n",
    "                    index=\"dbpdiea_type_centric\", q=question_processed, _source=True, size=30\n",
    "                )[\"hits\"][\"hits\"]\n",
    "                rank_list= [hit['_source'][\"type\"] for hit in hits]\n",
    "            \n",
    "\n",
    "                for DBOtype in rank_list:\n",
    "                    if DBOtype not in entry['type']:\n",
    "                        try:\n",
    "                            features=extract_features_baseline(question_processed,\n",
    "                                                                DBOtype,\n",
    "                                                                DBpedia_map_type_entities,\n",
    "                                                                docID_DBOtype_dict,\n",
    "                                                                map_docID_docLength,                                                \n",
    "                                                                collection_len,doc_number,avgdl,\n",
    "                                                                es)\n",
    "                        except BaseException as err:\n",
    "                            print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                            print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                            raise\n",
    "                            \n",
    "                        j+=1\n",
    "                        if j%500==0:\n",
    "                            print(\"-----------------\",datetime.datetime.now())\n",
    "                            print(\"--------------------j:\",j)\n",
    "                        writer.writerow(features)\n",
    "                        writer.writerow([0])\n",
    "\n",
    "\n",
    "        \n",
    "    print(f'total number {j} questions have been processed, in which {count} question has resource category')\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junec\\anaconda3\\lib\\site-packages\\elasticsearch\\connection\\base.py:209: ElasticsearchWarning: [types removal] Specifying types in term vector requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------count: 0\n",
      "------------error for type: dbo:Book Who is the pupil of the tutor Miguel √Ångel Estrella?\n",
      "Unexpected , <class 'KeyboardInterrupt'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepare_training_data_baseline(smarttask_dbpedia_train,\n",
    "                                        DBpedia_map_type_entities,\n",
    "                                        docID_DBOtype_dict,\n",
    "                                        map_docID_docLength,\n",
    "                                        es) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
