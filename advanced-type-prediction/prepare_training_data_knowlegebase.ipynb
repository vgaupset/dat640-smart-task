{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**need some %run command and uncomment the model load before submitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------run baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,json,re,string\n",
    "from typing import Callable, Dict, List, Set, Tuple\n",
    "from elasticsearch import Elasticsearch\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import datetime\n",
    "import csv\n",
    "sys.path.insert(0, 'extract_features')\n",
    "from extract_No1to5_features import extract_features_1to5\n",
    "from extract_No11to12_features import extract_features_11to12\n",
    "from extract_No13to15_features import TypeTaxonomy, extract_features_13to15\n",
    "from extract_No16_feature import extract_features_16\n",
    "from extract_No17to19_features import get_analyze,extract_features_17to19\n",
    "from extract_No20to22_features import extract_features_20to22\n",
    "from extract_No23to25_features_optimized import extract_features_23to25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging level.\n",
    "import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "logging.getLogger(\"elasticsearch\").disabled = True\n",
    "# es_log = logging.getLogger(\"elasticsearch\")\n",
    "# es_log.setLevel(logging.CRITICAL)\n",
    "es= Elasticsearch(timeout=600)\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run extract_features/extract_No11to12_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for my computer , need the following enviroment to run this script**\n",
    "<br>\n",
    "/c/Users/junec/anaconda3/python\n",
    "<br>\n",
    "/c/Users/junec/anaconda3/Scripts/pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files\n",
    "filepath=\"data/DBpedia_map_type_entities.json\"\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    DBpedia_map_type_entities = json.load(file)\n",
    "    \n",
    "filepath=\"data/training_types.json\"\n",
    "with open(filepath,encoding='utf-8') as json_file:\n",
    "    training_map_type_questions = json.load(json_file)\n",
    "\n",
    "filepath=\"data/ElasticSearch_map_type_docID.json\"\n",
    "with open(filepath, 'r',encoding='utf-8') as f:\n",
    "    docID_DBOtype_dict = json.load(f)\n",
    "\n",
    "filepath=\"../smart-dataset/datasets/DBpedia/smarttask_dbpedia_train.json\"\n",
    "with open(filepath, 'r') as f:\n",
    "    smarttask_dbpedia_train = json.load(f)\n",
    "\n",
    "typeobj=TypeTaxonomy(\"data/dbpedia_types.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_list=list(typeobj._types.keys())\n",
    "# type_list[1:]\n",
    "# len(type_list[1:]),len(smarttask_dbpedia_train) ,smarttask_dbpedia_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_type=\"dbo:MilitaryPerson\"\n",
    "# question=\"Does the shelf life of spinach equal 8?\"\n",
    "\n",
    "# dp_type=\"dbo:MusicFestival\"\n",
    "# question=\"When was Bibi Andersson music festival married to Per Ahlmark very green?\"\n",
    "\n",
    "# extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "# features_1to5=extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "# extract_features_11to12(dp_type, question,docID_DBOtype_dict,es)\n",
    "# extract_features_13to15(typeobj,dp_type)\n",
    "# print(extract_features_16(DBpedia_map_type_entities,dp_type))\n",
    "\n",
    "# extract_features_17to19(training_map_type_questions,dp_type,question)\n",
    "# extract_features_20to22(dp_type,question)\n",
    "# extract_features_23to25(model_loaded,dp_type, question, mode=\"Euclidean\")\n",
    "# extract_features_23to25(model_loaded,dp_type, question, mode=\"similarities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc: str) -> str:\n",
    "    \"\"\"Preprocesses text to prepare it for feature extraction.\n",
    "\n",
    "    Args:\n",
    "        doc: String comprising the unprocessed contents of some email file.\n",
    "\n",
    "    Returns:\n",
    "        String comprising the corresponding preprocessed text.\n",
    "    \"\"\"\n",
    "    re_html = re.compile(\"<[^>]+>\")\n",
    "    doc = re_html.sub(\" \", doc)\n",
    "    #remove pure digits \n",
    "    doc=re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\",\"\",doc)\n",
    "    # Replace punctuation marks (including hyphens) with spaces.\n",
    "    for c in string.punctuation:\n",
    "        doc = doc.replace(c, \" \")\n",
    "    #return doc.lower()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_baseline(\n",
    "    question:str,\n",
    "    dp_type: str,\n",
    "    DBpedia_map_type_entities:Dict,\n",
    "    docID_DBOtype_dict:Dict,\n",
    "    es: Elasticsearch,\n",
    " \n",
    ") -> List[float]:\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Args:\n",
    "            query: string.\n",
    "            dp_type: DBO type.\n",
    "            es: Elasticsearch object instance.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    features_1to5 = extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "    feature_vect = list(features_1to5.values())\n",
    "\n",
    "    features_11to12 = extract_features_11to12(dp_type, question,docID_DBOtype_dict,es)\n",
    "    feature_vect.extend(list(features_11to12.values()))\n",
    "    \n",
    "    return feature_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with first path filter\n",
    "def prepare_training_data_baseline( smarttask_dbpedia_train:List[Dict],\n",
    "                            DBpedia_map_type_entities:Dict[str,List],\n",
    "                            docID_DBOtype_dict:Dict[str,str],\n",
    "                            es: Elasticsearch\n",
    "                            )-> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "#     X_train=[]\n",
    "#     y_train=[]\n",
    "    all_DBOtype=list(typeobj._types.keys())\n",
    "    #remove the first type:\"thing\"\n",
    "    all_DBOtype=all_DBOtype[1:]\n",
    "    \n",
    "    count=0\n",
    "    j=0\n",
    "#     file_X = open(\"data/X_train_baseline.csv\", 'w', newline='')\n",
    "#     writer_X = csv.writer(file_X)\n",
    "#     file_y = open(\"data/y_train_baseline.csv\", 'w', newline='')\n",
    "#     writer_y = csv.writer(file_X)\n",
    "    with open(\"data/for_training_baseline.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "\n",
    "        for entry in smarttask_dbpedia_train:\n",
    "            if j%100==0:\n",
    "                print(\"--------------------j:\",j)\n",
    "\n",
    "            j+=1\n",
    "            if entry['question']==None:\n",
    "                continue\n",
    "\n",
    "            question_processed=preprocess(entry['question'])\n",
    "\n",
    "            if entry['category']=='resource':\n",
    "                if count%20==0:\n",
    "                    print(\"--------------------count:\",count)\n",
    "                    print(\"-----------------\",datetime.datetime.now())\n",
    "                count+=1\n",
    "                print(\"question_processed:\",question_processed)\n",
    "                for DBOtype in entry['type']: \n",
    "                    try:\n",
    "                        features=extract_features_baseline(question_processed,\n",
    "                                                            DBOtype,\n",
    "                                                            DBpedia_map_type_entities,\n",
    "                                                            docID_DBOtype_dict,\n",
    "                                                            es)\n",
    "                    except BaseException as err:\n",
    "                        print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                        print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                        raise\n",
    "#                     X_train.append(features)\n",
    "#                     y_train.append(1)\n",
    "                    writer.writerow(features)\n",
    "                    writer.writerow([1])\n",
    "\n",
    "\n",
    "    #                 writer_X.writerow(features)\n",
    "    #                 writer_y.writerow(y_train)\n",
    "\n",
    "                #deal with top 100 documents\n",
    "                hits = es.search(\n",
    "                    index=\"dbpdiea_type_centric\", q=question_processed, _source=True, size=30\n",
    "                )[\"hits\"][\"hits\"]\n",
    "                rank_list= [hit['_source'][\"type\"] for hit in hits]\n",
    "                #print(rank_list)\n",
    "\n",
    "                for DBOtype in rank_list:\n",
    "                    if DBOtype not in entry['type']:\n",
    "                        try:\n",
    "                            features=extract_features_baseline(question_processed,\n",
    "                                                                DBOtype,\n",
    "                                                                DBpedia_map_type_entities,\n",
    "                                                                docID_DBOtype_dict,\n",
    "                                                                es)\n",
    "                        except BaseException as err:\n",
    "                            print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                            print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                            raise\n",
    "                        #print(features)\n",
    "#                         X_train.append(features)\n",
    "#                         y_train.append(0)\n",
    "                        writer.writerow(features)\n",
    "                        writer.writerow([0])\n",
    "#                         writer_X.writerow(features)\n",
    "#                         writer_y.writerow(y_train)\n",
    "\n",
    "    #                 if count%100000==0:\n",
    "    #                     print(f'--------------{count} is finished')\n",
    "    #                     if len(X_train)%500==0:\n",
    "    #                         print(\"-----------------\",datetime.datetime.now())\n",
    "    #                         print(f'--------------length of X_train:{len(X_train)} y_train: {len(y_train)}')\n",
    "        \n",
    "    print(f'{count} features has been saved,{j} questions have been processed')\n",
    "#     file_X.close()\n",
    "#     file_y.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "prepare_training_data_baseline(smarttask_dbpedia_train,\n",
    "                                        DBpedia_map_type_entities,\n",
    "                                        docID_DBOtype_dict,\n",
    "                                        es) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #no first pass filter\n",
    "# def prepare_training_data_baseline( smarttask_dbpedia_train:List[Dict],\n",
    "#                             DBpedia_map_type_entities:Dict[str,List],\n",
    "#                             docID_DBOtype_dict:Dict[str,str],\n",
    "#                             es: Elasticsearch\n",
    "#                             )-> Tuple[List[List[float]], List[int]]:\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     X_train=[]\n",
    "#     y_train=[]\n",
    "#     all_DBOtype=list(typeobj._types.keys())\n",
    "#     #remove the first type:\"thing\"\n",
    "#     all_DBOtype=all_DBOtype[1:]\n",
    "    \n",
    "#     count=0\n",
    "#     j=0\n",
    "#     file_X = open(\"data/X_train_baseline.csv\", 'w', newline='')\n",
    "#     writer_X = csv.writer(file_X)\n",
    "#     file_y = open(\"data/y_train_baseline.csv\", 'w', newline='')\n",
    "#     writer_y = csv.writer(file_X)\n",
    "# #     with open(\"data/baseline_features.csv\", 'w', newline='') as csvfile:\n",
    "# #         writer = csv.writer(csvfile)\n",
    "\n",
    "\n",
    "#     for entry in smarttask_dbpedia_train:\n",
    "# #         if j%int(len(smarttask_dbpedia_train)/100)==0:\n",
    "# #             print(f'--------------{j/int(len(smarttask_dbpedia_train)/100)}% is finished')\n",
    "#         if j%100==0:\n",
    "#             print(\"--------------------j:\",j)\n",
    "\n",
    "#         j+=1\n",
    "#         if entry['question']==None:\n",
    "#             continue\n",
    "            \n",
    "#         question_processed=preprocess(entry['question'])\n",
    "        \n",
    "#         if entry['category']=='resource':\n",
    "#             for DBOtype in all_DBOtype: \n",
    "#                 try:\n",
    "#                     features=extract_features_baseline(question_processed,\n",
    "#                                                         DBOtype,\n",
    "#                                                         DBpedia_map_type_entities,\n",
    "#                                                         docID_DBOtype_dict,\n",
    "#                                                         es)\n",
    "#                 except BaseException as err:\n",
    "#                     print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "#                     print(f\"Unexpected {err}, {type(err)}\")  \n",
    "#                     raise\n",
    "#                 X_train.append(features)\n",
    "#                 count+=1\n",
    "#                 #print(f'--------------length of X_train:{len(X_train)} y_train: {len(y_train)}')\n",
    "#                 if DBOtype in entry['type']: \n",
    "#                     y_train.append(1)\n",
    "#                 else:\n",
    "#                     y_train.append(0)\n",
    "\n",
    "\n",
    "#                 writer_X.writerow(features)\n",
    "#                 writer_y.writerow(y_train)\n",
    "\n",
    "#                 if count%100000==0:\n",
    "#                     print(f'--------------{count} is finished')\n",
    "#                 if len(X_train)%500==0:\n",
    "#                     print(\"-----------------\",datetime.datetime.now())\n",
    "#                     print(f'--------------length of X_train:{len(X_train)} y_train: {len(y_train)}')\n",
    "        \n",
    "#     print(f'{count} features has been saved,{j} questions have been processed')\n",
    "#     file_X.close()\n",
    "#     file_y.close()\n",
    "#     return X_train,y_train\n",
    "\n",
    "\n",
    "\n",
    "# X_train,y_train=prepare_training_data_baseline(smarttask_dbpedia_train,\n",
    "#                                         DBpedia_map_type_entities,\n",
    "#                                         docID_DBOtype_dict,\n",
    "#                                         es) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/X_train_baseline.json\", 'w') as f:\n",
    "#   json.dump(X_train, f)\n",
    "# with open(\"data/y_train_baseline.json\", 'w') as f:\n",
    "#   json.dump(y_train, f)\n",
    "# print(\"X_train,y_train are saved as json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/baseline_features.csv\")\n",
    "csvreader = csv.reader(file)\n",
    "rows = []\n",
    "for row in csvreader:\n",
    "        rows.append(row)\n",
    "rows\n",
    "\n",
    "type(rows[3])\n",
    "rows[3]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
