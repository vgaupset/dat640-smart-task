{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import modules and set up logging.\n",
    "from typing import Callable, Dict, List, Set, Tuple, Generator\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to download the trained model of 'word2vec-google-news-300'\n",
    "#make sure to use a 64 bit python\n",
    "import struct\n",
    "struct.calcsize(\"P\") * 8\n",
    "#!which python\n",
    "#!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging level.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 01:31:59,491 : INFO : loading projection weights from C:\\Users\\junec/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2021-11-10 01:32:47,777 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\junec/gensim-data\\\\word2vec-google-news-300\\\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-11-10T01:32:47.773867', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "model_loaded = api.load('word2vec-google-news-300')\n",
    "#model_loaded.save('googleNews.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/57507832/unable-to-allocate-array-with-shape-and-data-type\n",
    "#model_loaded = gensim.models.keyedvectors.KeyedVectors.load('googleNews.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_WORD_TYPE=[\"noun\",\"adj\",\"verb\",\"adv\"]\n",
    "POS_TAGS=[\"NN\",\"JJ\",\"VB\",\"RB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessDBOtype(dp_type:str)->str:\n",
    "    dp_type=dp_type[len(\"dbo:\"):]\n",
    "    splitted_type=re.findall('[A-Z][a-z]*', dp_type)\n",
    "    return \" \".join(splitted_type).lower()\n",
    "    #return \" \".join(splitted_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_word_POStag(sentence:str)->Dict:\n",
    "    \"\"\"\n",
    "    parse content words of a sentence with their POS tag\n",
    "    argument:a sentence string\n",
    "    return:dictionary,key is POS tag and value is the corresponding word\n",
    "           a list of content words\n",
    "    \"\"\"\n",
    "    tag_dict={}\n",
    "    content_words=[]\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    tagged_words=nltk.pos_tag(tokenized_sentence)\n",
    "    for POS_tag in POS_TAGS:\n",
    "        for word,tag in tagged_words:\n",
    "            if tag[:2]==POS_tag:\n",
    "                content_words.append(word)\n",
    "                temp=tag_dict.get(POS_tag,[])\n",
    "                temp.append(word)\n",
    "                tag_dict[POS_tag]=temp\n",
    "                #print(tag_dict[POS_tag])\n",
    "    return tag_dict,content_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pairwise_similarity(question_tagged:Dict,type_tagged:Dict)->List:\n",
    "    \"\"\"\n",
    "    calculate pairwise similarity between \n",
    "    content words in the query and the type label\n",
    "    \"\"\"\n",
    "    similarities=[]\n",
    "    for POS_tag,words in type_tagged.items():\n",
    "        if POS_tag in question_tagged.keys():\n",
    "            for word1 in words:\n",
    "                for word2 in question_tagged[POS_tag]:\n",
    "                    similarities.append(model_loaded.similarity(word1,word2))\n",
    "    return similarities\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_23to25(dp_type:str, question:str)->Tuple[float,float,float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #get content words and parse dictonary\n",
    "    question_tagged,question_content=parse_word_POStag(question)\n",
    "    processed_type=preprocessDBOtype(dp_type)\n",
    "    type_tagged,type_content=parse_word_POStag(processed_type)\n",
    "    #get centroid\n",
    "    question_centrality=model_loaded.rank_by_centrality(question_content, use_norm=True)\n",
    "    type_centrality=model_loaded.rank_by_centrality(type_content, use_norm=True)\n",
    "    question_centroid=question_centrality[0][1]\n",
    "    type_centroid=type_centrality[0][1]\n",
    "    #feature 23\n",
    "    sim_aggr=round(model_loaded.similarity(question_centroid, type_centroid),4)\n",
    "    \n",
    "    pairwise_similarity=calc_pairwise_similarity(question_tagged,type_tagged)\n",
    "    #feature 24,25\n",
    "    sim_max=max(pairwise_similarity)\n",
    "    sim_avg=round(sum(pairwise_similarity)/len(pairwise_similarity),4)\n",
    "    \n",
    "    return sim_aggr,sim_max,sim_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0196, 0.13620232, 0.0312)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_type=\"dbo:GreatMusicFestival\"\n",
    "question=\"When was Bibi Andersson married to Per Ahlmark very green?\"\n",
    "extract_feature_23to25(dp_type, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
