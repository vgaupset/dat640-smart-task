{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**need some %run command and uncomment the model load before submitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys,json,re,string\n",
    "from typing import Callable, Dict, List, Set, Tuple\n",
    "from elasticsearch import Elasticsearch\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import datetime\n",
    "import csv\n",
    "sys.path.insert(0, 'extract_features')\n",
    "from extract_No1to5_features import extract_features_1to5\n",
    "from extract_No11to12_features import extract_features_11to12\n",
    "from extract_No13to15_features import TypeTaxonomy, extract_features_13to15\n",
    "from extract_No16_feature import extract_features_16\n",
    "from extract_No17to19_features import get_analyze,extract_features_17to19\n",
    "from extract_No20to22_features import extract_features_20to22\n",
    "from extract_No23to25_features_optimized import extract_features_23to25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'LAPTOP-ADBLIUPR',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': '5VELmBwJTk-urTuhZdTgew',\n",
       " 'version': {'number': '7.15.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': '83c34f456ae29d60e94d886e455e6a3409bba9ed',\n",
       "  'build_date': '2021-10-07T21:56:19.031608185Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.9.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce logging level.\n",
    "import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "logging.getLogger(\"elasticsearch\").disabled = True\n",
    "# es_log = logging.getLogger(\"elasticsearch\")\n",
    "# es_log.setLevel(logging.CRITICAL)\n",
    "es= Elasticsearch(timeout=600)\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run extract_features/extract_No11to12_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for my computer , need the following enviroment to run this script**\n",
    "<br>\n",
    "/c/Users/junec/anaconda3/python\n",
    "<br>\n",
    "/c/Users/junec/anaconda3/Scripts/pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/junec/anaconda3/python\n",
      "/c/Users/junec/anaconda3/Scripts/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 06:41:46,913 : INFO : loading KeyedVectors object from googleNews.d2v\n",
      "2021-11-20 06:41:48,353 : INFO : loading vectors from googleNews.d2v.vectors.npy with mmap=None\n",
      "2021-11-20 06:42:08,299 : INFO : KeyedVectors lifecycle event {'fname': 'googleNews.d2v', 'datetime': '2021-11-20T06:42:08.267898', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "try:\n",
    "    model_loaded = gensim.models.keyedvectors.KeyedVectors.load('googleNews.d2v')\n",
    "except:\n",
    "    model_loaded = api.load('word2vec-google-news-300')\n",
    "    model_loaded.save('googleNews.d2v')\n",
    "    model_loaded = gensim.models.keyedvectors.KeyedVectors.load('googleNews.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files\n",
    "filepath=\"data/DBpedia_map_type_entities.json\"\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    DBpedia_map_type_entities = json.load(file)\n",
    "    \n",
    "filepath=\"data/training_types.json\"\n",
    "with open(filepath,encoding='utf-8') as json_file:\n",
    "    training_map_type_questions = json.load(json_file)\n",
    "\n",
    "filepath=\"data/ElasticSearch_map_type_docID.json\"\n",
    "with open(filepath, 'r',encoding='utf-8') as f:\n",
    "    docID_DBOtype_dict = json.load(f)\n",
    "\n",
    "filepath=\"../smart-dataset/datasets/DBpedia/smarttask_dbpedia_train.json\"\n",
    "with open(filepath, 'r') as f:\n",
    "    smarttask_dbpedia_train = json.load(f)\n",
    "\n",
    "typeobj=TypeTaxonomy(\"data/dbpedia_types.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_list=list(typeobj._types.keys())\n",
    "# type_list[1:]\n",
    "# len(type_list[1:]),len(smarttask_dbpedia_train) ,smarttask_dbpedia_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_type=\"dbo:MilitaryPerson\"\n",
    "# question=\"Does the shelf life of spinach equal 8?\"\n",
    "\n",
    "# dp_type=\"dbo:MusicFestival\"\n",
    "# question=\"When was Bibi Andersson music festival married to Per Ahlmark very green?\"\n",
    "\n",
    "# extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "# features_1to5=extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "# extract_features_11to12(dp_type, question,docID_DBOtype_dict,es)\n",
    "# extract_features_13to15(typeobj,dp_type)\n",
    "# print(extract_features_16(DBpedia_map_type_entities,dp_type))\n",
    "\n",
    "# extract_features_17to19(training_map_type_questions,dp_type,question)\n",
    "# extract_features_20to22(dp_type,question)\n",
    "# extract_features_23to25(model_loaded,dp_type, question, mode=\"Euclidean\")\n",
    "# extract_features_23to25(model_loaded,dp_type, question, mode=\"similarities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc: str) -> str:\n",
    "    \"\"\"Preprocesses text to prepare it for feature extraction.\n",
    "\n",
    "    Args:\n",
    "        doc: String comprising the unprocessed contents of some email file.\n",
    "\n",
    "    Returns:\n",
    "        String comprising the corresponding preprocessed text.\n",
    "    \"\"\"\n",
    "    re_html = re.compile(\"<[^>]+>\")\n",
    "    doc = re_html.sub(\" \", doc)\n",
    "    #remove pure digits \n",
    "    doc=re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\",\"\",doc)\n",
    "    # Replace punctuation marks (including hyphens) with spaces.\n",
    "    for c in string.punctuation:\n",
    "        doc = doc.replace(c, \" \")\n",
    "    #return doc.lower()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_type_label(\n",
    "    question:str,\n",
    "    dp_type: str,\n",
    "    training_map_type_questions:Dict,\n",
    "    model_loaded:gensim.models.keyedvectors.KeyedVectors,\n",
    "    es: Elasticsearch,\n",
    "    analyze,X,terms_corpus,\n",
    "    add_extra_features:bool=False\n",
    ") -> List[float]:\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Args:\n",
    "            query: string.\n",
    "            dp_type: DBO type.\n",
    "            es: Elasticsearch object instance.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"  \n",
    "    features_17to19=extract_features_17to19(analyze,X,terms_corpus,training_map_type_questions,dp_type,question)\n",
    "    feature_vect=list(features_17to19.values())\n",
    "    \n",
    "    features_20to22=extract_features_20to22(dp_type,question)\n",
    "    feature_vect.extend(list(features_20to22.values()))\n",
    "    \n",
    "    features_23to25=extract_features_23to25(model_loaded,dp_type, question, mode=\"Euclidean\")\n",
    "    feature_vect.extend(list(features_23to25.values()))\n",
    "\n",
    "    if add_extra_features:\n",
    "        features_23to25_variant=extract_features_23to25(model_loaded,dp_type, question, mode=\"similarities\")\n",
    "        feature_vect.extend(list(features_23to25_variant.values()))\n",
    "    \n",
    "    return feature_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze,X,terms_corpus=get_analyze(training_map_type_questions)\n",
    "# DBOtype=\"dbo:MusicFestival\"\n",
    "# question=\"When was Bibi Andersson music festival married to Per Ahlmark very green?\"\n",
    "# question=\"Who is {famous for} of {writers} of {To the Christian Nobility of the German Nation} ?\"\n",
    "# extract_features_type_label(question,\n",
    "#                                                         DBOtype,\n",
    "#                                                         training_map_type_questions,\n",
    "#                                                         model_loaded,\n",
    "#                                                         es,\n",
    "#                                                         analyze,X,terms_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"data/baseline_features.csv\")\n",
    "# csvreader = csv.reader(file)\n",
    "# rows = []\n",
    "# for row in csvreader:\n",
    "#         rows.append(row)\n",
    "# rows\n",
    "\n",
    "# type(rows[3])\n",
    "# rows[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------j: 0\n",
      "----------------- 2021-11-20 06:42:12.250400\n",
      "--------------------count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junec\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\junec\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- 2021-11-20 06:45:28.287709\n",
      "--------------------count: 30\n",
      "--------------------j: 100\n",
      "----------------- 2021-11-20 06:48:19.671073\n",
      "--------------------count: 60\n",
      "----------------- 2021-11-20 06:50:41.771298\n",
      "--------------------count: 90\n",
      "--------------------j: 200\n",
      "----------------- 2021-11-20 06:52:39.138388\n",
      "--------------------count: 120\n",
      "----------------- 2021-11-20 06:55:40.045112\n",
      "--------------------count: 150\n",
      "--------------------j: 300\n",
      "----------------- 2021-11-20 06:57:42.145476\n",
      "--------------------count: 180\n",
      "----------------- 2021-11-20 07:00:42.011795\n",
      "--------------------count: 210\n",
      "--------------------j: 400\n",
      "----------------- 2021-11-20 07:03:02.496809\n",
      "--------------------count: 240\n",
      "----------------- 2021-11-20 07:05:32.707688\n",
      "--------------------count: 270\n",
      "--------------------j: 500\n",
      "----------------- 2021-11-20 07:08:14.221916\n",
      "--------------------count: 300\n",
      "----------------- 2021-11-20 07:11:02.133023\n",
      "--------------------count: 330\n",
      "--------------------j: 600\n",
      "----------------- 2021-11-20 07:14:24.478952\n",
      "--------------------count: 360\n",
      "----------------- 2021-11-20 07:16:59.152115\n",
      "--------------------count: 390\n",
      "--------------------j: 700\n",
      "----------------- 2021-11-20 07:19:32.830771\n",
      "--------------------count: 420\n",
      "----------------- 2021-11-20 07:21:42.863283\n",
      "--------------------count: 450\n",
      "--------------------j: 800\n",
      "----------------- 2021-11-20 07:24:24.309505\n",
      "--------------------count: 480\n",
      "--------------------j: 900\n",
      "----------------- 2021-11-20 07:27:01.660233\n",
      "--------------------count: 510\n",
      "----------------- 2021-11-20 07:29:30.383267\n",
      "--------------------count: 540\n",
      "--------------------j: 1000\n",
      "----------------- 2021-11-20 07:31:59.020735\n",
      "--------------------count: 570\n",
      "----------------- 2021-11-20 07:34:44.138586\n",
      "--------------------count: 600\n",
      "--------------------j: 1100\n",
      "----------------- 2021-11-20 07:36:34.066714\n",
      "--------------------count: 630\n",
      "----------------- 2021-11-20 07:39:14.035743\n",
      "--------------------count: 660\n",
      "--------------------j: 1200\n",
      "----------------- 2021-11-20 07:41:46.385751\n",
      "--------------------count: 690\n",
      "----------------- 2021-11-20 07:44:12.777623\n",
      "--------------------count: 720\n",
      "--------------------j: 1300\n",
      "----------------- 2021-11-20 07:46:34.434484\n",
      "--------------------count: 750\n",
      "--------------------j: 1400\n",
      "----------------- 2021-11-20 07:48:37.986116\n",
      "--------------------count: 780\n",
      "----------------- 2021-11-20 07:50:54.944613\n",
      "--------------------count: 810\n",
      "----------------- 2021-11-20 07:53:38.865300\n",
      "--------------------count: 840\n",
      "--------------------j: 1500\n",
      "----------------- 2021-11-20 07:55:59.040504\n",
      "--------------------count: 870\n",
      "--------------------j: 1600\n",
      "----------------- 2021-11-20 07:58:32.064050\n",
      "--------------------count: 900\n",
      "----------------- 2021-11-20 08:01:24.430274\n",
      "--------------------count: 930\n",
      "--------------------j: 1700\n",
      "----------------- 2021-11-20 08:04:30.152333\n",
      "--------------------count: 960\n",
      "----------------- 2021-11-20 08:06:47.281646\n",
      "--------------------count: 990\n",
      "--------------------j: 1800\n",
      "----------------- 2021-11-20 08:09:52.497247\n",
      "--------------------count: 1020\n",
      "--------------------j: 1900\n",
      "----------------- 2021-11-20 08:12:22.664465\n",
      "--------------------count: 1050\n",
      "----------------- 2021-11-20 08:14:41.027258\n",
      "--------------------count: 1080\n",
      "--------------------j: 2000\n",
      "----------------- 2021-11-20 08:17:06.487245\n",
      "--------------------count: 1110\n",
      "--------------------j: 2100\n",
      "----------------- 2021-11-20 08:19:38.774893\n",
      "--------------------count: 1140\n",
      "----------------- 2021-11-20 08:21:58.419787\n",
      "--------------------count: 1170\n",
      "--------------------j: 2200\n",
      "----------------- 2021-11-20 08:24:55.450046\n",
      "--------------------count: 1200\n",
      "----------------- 2021-11-20 08:27:36.809631\n",
      "--------------------count: 1230\n",
      "--------------------j: 2300\n",
      "----------------- 2021-11-20 08:30:07.793828\n",
      "--------------------count: 1260\n",
      "--------------------j: 2400\n",
      "----------------- 2021-11-20 08:33:21.843005\n",
      "--------------------count: 1290\n",
      "----------------- 2021-11-20 08:35:48.101349\n",
      "--------------------count: 1320\n",
      "--------------------j: 2500\n",
      "----------------- 2021-11-20 08:38:06.327076\n",
      "--------------------count: 1350\n",
      "----------------- 2021-11-20 08:40:09.516777\n",
      "--------------------count: 1380\n",
      "--------------------j: 2600\n",
      "----------------- 2021-11-20 08:42:31.145199\n",
      "--------------------count: 1410\n",
      "----------------- 2021-11-20 08:45:09.400073\n",
      "--------------------count: 1440\n",
      "--------------------j: 2700\n",
      "----------------- 2021-11-20 08:47:40.074089\n",
      "--------------------count: 1470\n",
      "----------------- 2021-11-20 08:49:56.854545\n",
      "--------------------count: 1500\n",
      "--------------------j: 2800\n",
      "----------------- 2021-11-20 08:52:21.626844\n",
      "--------------------count: 1530\n",
      "----------------- 2021-11-20 08:54:24.812076\n",
      "--------------------count: 1560\n",
      "--------------------j: 2900\n",
      "----------------- 2021-11-20 08:56:47.352569\n",
      "--------------------count: 1590\n",
      "----------------- 2021-11-20 08:59:20.116588\n",
      "--------------------count: 1620\n",
      "--------------------j: 3000\n",
      "----------------- 2021-11-20 09:02:02.288968\n",
      "--------------------count: 1650\n",
      "----------------- 2021-11-20 09:04:08.591264\n",
      "--------------------count: 1680\n",
      "--------------------j: 3100\n",
      "----------------- 2021-11-20 09:06:35.792918\n",
      "--------------------count: 1710\n",
      "----------------- 2021-11-20 09:08:59.339791\n",
      "--------------------count: 1740\n",
      "--------------------j: 3200\n",
      "----------------- 2021-11-20 09:11:48.164153\n",
      "--------------------count: 1770\n",
      "----------------- 2021-11-20 09:14:51.089604\n",
      "--------------------count: 1800\n",
      "--------------------j: 3300\n",
      "----------------- 2021-11-20 09:17:36.749906\n",
      "--------------------count: 1830\n",
      "----------------- 2021-11-20 09:20:36.807680\n",
      "--------------------count: 1860\n",
      "--------------------j: 3400\n",
      "----------------- 2021-11-20 09:23:09.909282\n",
      "--------------------count: 1890\n",
      "--------------------j: 3500\n",
      "----------------- 2021-11-20 09:25:46.949009\n",
      "--------------------count: 1920\n",
      "----------------- 2021-11-20 09:28:14.108765\n",
      "--------------------count: 1950\n",
      "--------------------j: 3600\n",
      "----------------- 2021-11-20 09:30:14.299477\n",
      "--------------------count: 1980\n",
      "----------------- 2021-11-20 09:33:09.507767\n",
      "--------------------count: 2010\n",
      "--------------------j: 3700\n",
      "----------------- 2021-11-20 09:36:07.811829\n",
      "--------------------count: 2040\n",
      "----------------- 2021-11-20 09:38:57.644663\n",
      "--------------------count: 2070\n",
      "--------------------j: 3800\n",
      "----------------- 2021-11-20 09:41:41.743053\n",
      "--------------------count: 2100\n",
      "----------------- 2021-11-20 09:43:37.923832\n",
      "--------------------count: 2130\n",
      "--------------------j: 3900\n",
      "----------------- 2021-11-20 09:46:43.090066\n",
      "--------------------count: 2160\n",
      "--------------------j: 4000\n",
      "----------------- 2021-11-20 09:49:22.028693\n",
      "--------------------count: 2190\n",
      "----------------- 2021-11-20 09:52:04.986351\n",
      "--------------------count: 2220\n",
      "--------------------j: 4100\n",
      "----------------- 2021-11-20 09:54:09.330680\n",
      "--------------------count: 2250\n",
      "--------------------j: 4200\n",
      "----------------- 2021-11-20 09:56:25.828732\n",
      "--------------------count: 2280\n",
      "----------------- 2021-11-20 09:58:56.619082\n",
      "--------------------count: 2310\n",
      "--------------------j: 4300\n",
      "----------------- 2021-11-20 10:01:41.649079\n",
      "--------------------count: 2340\n",
      "----------------- 2021-11-20 10:04:30.907742\n",
      "--------------------count: 2370\n",
      "--------------------j: 4400\n",
      "----------------- 2021-11-20 10:06:52.214722\n",
      "--------------------count: 2400\n",
      "----------------- 2021-11-20 10:09:17.064738\n",
      "--------------------count: 2430\n",
      "--------------------j: 4500\n",
      "----------------- 2021-11-20 10:11:31.573208\n",
      "--------------------count: 2460\n",
      "----------------- 2021-11-20 10:14:33.096861\n",
      "--------------------count: 2490\n",
      "--------------------j: 4600\n",
      "----------------- 2021-11-20 10:16:50.151423\n",
      "--------------------count: 2520\n",
      "----------------- 2021-11-20 10:19:28.787069\n",
      "--------------------count: 2550\n",
      "--------------------j: 4700\n",
      "----------------- 2021-11-20 10:21:59.730103\n",
      "--------------------count: 2580\n",
      "--------------------j: 4800\n",
      "----------------- 2021-11-20 10:24:43.968063\n",
      "--------------------count: 2610\n",
      "----------------- 2021-11-20 10:27:06.438840\n",
      "--------------------count: 2640\n",
      "--------------------j: 4900\n",
      "----------------- 2021-11-20 10:29:46.568946\n",
      "--------------------count: 2670\n",
      "----------------- 2021-11-20 10:32:22.924831\n",
      "--------------------count: 2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------j: 5000\n",
      "----------------- 2021-11-20 10:34:45.248544\n",
      "--------------------count: 2730\n",
      "----------------- 2021-11-20 10:36:58.418805\n",
      "--------------------count: 2760\n",
      "--------------------j: 5100\n",
      "----------------- 2021-11-20 10:39:43.734570\n",
      "--------------------count: 2790\n",
      "----------------- 2021-11-20 10:42:25.007195\n",
      "--------------------count: 2820\n",
      "--------------------j: 5200\n",
      "----------------- 2021-11-20 10:44:49.134152\n",
      "--------------------count: 2850\n",
      "--------------------j: 5300\n",
      "----------------- 2021-11-20 10:47:14.321605\n",
      "--------------------count: 2880\n",
      "----------------- 2021-11-20 10:49:57.401339\n",
      "--------------------count: 2910\n",
      "--------------------j: 5400\n",
      "----------------- 2021-11-20 10:52:11.113223\n",
      "--------------------count: 2940\n",
      "----------------- 2021-11-20 10:54:25.981928\n",
      "--------------------count: 2970\n",
      "--------------------j: 5500\n",
      "----------------- 2021-11-20 10:57:11.950312\n",
      "--------------------count: 3000\n",
      "--------------------j: 5600\n",
      "----------------- 2021-11-20 10:59:39.802668\n",
      "--------------------count: 3030\n",
      "----------------- 2021-11-20 11:02:11.108979\n",
      "--------------------count: 3060\n",
      "--------------------j: 5700\n",
      "----------------- 2021-11-20 11:05:07.728449\n",
      "--------------------count: 3090\n",
      "----------------- 2021-11-20 11:07:30.365985\n",
      "--------------------count: 3120\n",
      "--------------------j: 5800\n",
      "----------------- 2021-11-20 11:10:21.179309\n",
      "--------------------count: 3150\n",
      "----------------- 2021-11-20 11:12:55.805679\n",
      "--------------------count: 3180\n",
      "--------------------j: 5900\n",
      "----------------- 2021-11-20 11:15:11.383931\n",
      "--------------------count: 3210\n",
      "----------------- 2021-11-20 11:17:47.114755\n",
      "--------------------count: 3240\n",
      "--------------------j: 6000\n",
      "----------------- 2021-11-20 11:20:32.356670\n",
      "--------------------count: 3270\n",
      "----------------- 2021-11-20 11:23:06.770282\n",
      "--------------------count: 3300\n",
      "--------------------j: 6100\n",
      "----------------- 2021-11-20 11:25:58.324084\n",
      "--------------------count: 3330\n",
      "----------------- 2021-11-20 11:28:44.090242\n",
      "--------------------count: 3360\n",
      "--------------------j: 6200\n",
      "----------------- 2021-11-20 11:31:16.551607\n",
      "--------------------count: 3390\n",
      "--------------------j: 6300\n",
      "----------------- 2021-11-20 11:34:17.612575\n",
      "--------------------count: 3420\n",
      "----------------- 2021-11-20 11:37:39.896012\n",
      "--------------------count: 3450\n",
      "--------------------j: 6400\n",
      "----------------- 2021-11-20 11:39:57.293129\n",
      "--------------------count: 3480\n",
      "----------------- 2021-11-20 11:42:53.242817\n",
      "--------------------count: 3510\n",
      "--------------------j: 6500\n",
      "----------------- 2021-11-20 11:45:38.082358\n",
      "--------------------count: 3540\n",
      "--------------------j: 6600\n",
      "----------------- 2021-11-20 11:48:52.551658\n",
      "--------------------count: 3570\n",
      "----------------- 2021-11-20 11:52:27.574245\n",
      "--------------------count: 3600\n",
      "--------------------j: 6700\n",
      "----------------- 2021-11-20 11:55:38.611737\n",
      "--------------------count: 3630\n",
      "----------------- 2021-11-20 11:59:06.860162\n",
      "--------------------count: 3660\n",
      "--------------------j: 6800\n",
      "----------------- 2021-11-20 12:01:40.813109\n",
      "--------------------count: 3690\n"
     ]
    }
   ],
   "source": [
    "def prepare_training_data_type_label(  smarttask_dbpedia_train:List[Dict],\n",
    "                                        training_map_type_questions:Dict[str,str],\n",
    "                                        model_loaded:gensim.models.keyedvectors.KeyedVectors,\n",
    "                                        es: Elasticsearch,\n",
    "                                        add_extra_features:bool=False)-> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "#     X_train=[]\n",
    "#     y_train=[]\n",
    "    all_DBOtype=list(typeobj._types.keys())\n",
    "    #remove the first type:\"thing\"\n",
    "    all_DBOtype=all_DBOtype[1:]\n",
    "    analyze,X,terms_corpus=get_analyze(training_map_type_questions)\n",
    "    \n",
    "    count=0\n",
    "    j=0\n",
    "#     file_X = open(\"data/X_train_type_label.csv\", 'w', newline='')\n",
    "#     writer_X = csv.writer(file_X)\n",
    "#     file_y = open(\"data/y_train_type_label.csv\", 'w', newline='')\n",
    "#     writer_y = csv.writer(file_X)\n",
    "    with open(\"data/training_type_label.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "\n",
    "        for entry in smarttask_dbpedia_train:\n",
    "            if j%100==0:\n",
    "                print(\"--------------------j:\",j)\n",
    "\n",
    "            j+=1\n",
    "            if entry['question']==None:\n",
    "                continue\n",
    "\n",
    "            question_processed=preprocess(entry['question'])\n",
    "\n",
    "            if entry['category']=='resource':\n",
    "                if count%30==0:\n",
    "                    print(\"-----------------\",datetime.datetime.now())\n",
    "                    print(\"--------------------count:\",count)  \n",
    "#                   print(f'--------------length of X_train:{len(X_train)} y_train: {len(y_train)}')\n",
    "                count+=1\n",
    "                #print(\"question_processed:\",question_processed)\n",
    "                for DBOtype in entry['type']: \n",
    "                    try:\n",
    "                        features=extract_features_type_label(question_processed,\n",
    "                                                            DBOtype,\n",
    "                                                            training_map_type_questions,\n",
    "                                                            model_loaded,\n",
    "                                                            es,\n",
    "                                                            analyze,X,terms_corpus) \n",
    "                    except BaseException as err:\n",
    "                        print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                        print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                        raise\n",
    "#                     X_train.append(features)\n",
    "#                     y_train.append(1)\n",
    "\n",
    "\n",
    "#                     writer_X.writerow(features)\n",
    "#                     writer_y.writerow(y_train)\n",
    "                    writer.writerow(features)\n",
    "                    writer.writerow([1])\n",
    "\n",
    "                #deal with top 100 documents\n",
    "                hits = es.search(\n",
    "                    index=\"dbpdiea_type_centric\", q=question_processed, _source=True, size=30\n",
    "                )[\"hits\"][\"hits\"]\n",
    "                rank_list= [hit['_source'][\"type\"] for hit in hits]\n",
    "                #print(rank_list)\n",
    "\n",
    "                for DBOtype in rank_list:\n",
    "                    if DBOtype not in entry['type']:\n",
    "                        try:\n",
    "                            features=extract_features_type_label(question_processed,\n",
    "                                                            DBOtype,\n",
    "                                                            training_map_type_questions,\n",
    "                                                            model_loaded,\n",
    "                                                            es,\n",
    "                                                            analyze,X,terms_corpus) \n",
    "                        except BaseException as err:\n",
    "                            print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                            print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                            raise\n",
    "                            \n",
    "                        writer.writerow(features)\n",
    "                        writer.writerow([0])\n",
    "                        #print(features)\n",
    "#                         X_train.append(features)\n",
    "#                         y_train.append(0)\n",
    "#                         writer_X.writerow(features)\n",
    "#                         writer_y.writerow(y_train)\n",
    "\n",
    "    #                 if count%100000==0:\n",
    "    #                     print(f'--------------{count} is finished')\n",
    "    #                     if len(X_train)%500==0:\n",
    "    #                         print(\"-----------------\",datetime.datetime.now())\n",
    "    #                         print(f'--------------length of X_train:{len(X_train)} y_train: {len(y_train)}')\n",
    "\n",
    "    print(f'{count} features has been saved,{j} questions have been processed')\n",
    "#     file_X.close()\n",
    "#     file_y.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "prepare_training_data_type_label(smarttask_dbpedia_train,\n",
    "                                        training_map_type_questions,\n",
    "                                        model_loaded,\n",
    "                                        es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prepare_training_data_baseline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/X_train_type_label.json\", 'w') as f:\n",
    "#   json.dump(X_train, f)\n",
    "# with open(\"../data/X_train_type_label.json\", 'w') as f:\n",
    "#   json.dump(y_train, f)\n",
    "# print(\"X_train,y_train are saved as json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es=Elasticsearch()\n",
    "# es.get(index='dbpdiea_type_centric',id=\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index='dbpdiea_type_centric'\n",
    "# field=\"abstract\"\n",
    "# tv = es.termvectors(index=index, doc_type=\"_doc\", id=\"1\", fields=field, term_statistics=True) \n",
    "# collection_len=tv[\"term_vectors\"][field]['field_statistics']['sum_ttf']  \n",
    "# #total number of entity in the collections\n",
    "# doc_number=tv[\"term_vectors\"][field]['field_statistics']['doc_count'] \n",
    "# #average document length\n",
    "# avgdl=collection_len/doc_number\n",
    "# collection_len,doc_number,avgdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word=\"theatre\"\n",
    "# #model_loaded[word]\n",
    "# model_loaded.most_similar(positive=[word], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
