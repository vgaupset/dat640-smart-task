{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**need some %run command and uncomment the model load before submitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys,json\n",
    "from typing import Callable, Dict, List, Set, Tuple\n",
    "from elasticsearch import Elasticsearch\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import datetime\n",
    "sys.path.insert(0, 'extract_features')\n",
    "from extract_No1to5_features import extract_features_1to5\n",
    "from extract_No11to12_features import extract_features_11to12\n",
    "from extract_No13to15_features import TypeTaxonomy, extract_features_13to15\n",
    "from extract_No16_feature import extract_features_16\n",
    "from extract_No17to19_features import get_analyze,extract_features_17to19\n",
    "from extract_No20to22_features import extract_features_20to22\n",
    "from extract_No23to25_features_optimized import extract_features_23to25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'LAPTOP-ADBLIUPR',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': '5VELmBwJTk-urTuhZdTgew',\n",
       " 'version': {'number': '7.15.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': '83c34f456ae29d60e94d886e455e6a3409bba9ed',\n",
       "  'build_date': '2021-10-07T21:56:19.031608185Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.9.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce logging level.\n",
    "import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "logging.getLogger(\"elasticsearch\").disabled = True\n",
    "# es_log = logging.getLogger(\"elasticsearch\")\n",
    "# es_log.setLevel(logging.CRITICAL)\n",
    "es= Elasticsearch()\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run extract_features/extract_No11to12_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for my computer , need the following enviroment to run this script**\n",
    "<br>\n",
    "/c/Users/junec/anaconda3/python\n",
    "<br>\n",
    "/c/Users/junec/anaconda3/Scripts/pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/junec/anaconda3/python\n",
      "/c/Users/junec/anaconda3/Scripts/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 23:49:43,437 : INFO : loading KeyedVectors object from googleNews.d2v\n",
      "2021-11-19 23:49:46,252 : INFO : loading vectors from googleNews.d2v.vectors.npy with mmap=None\n",
      "2021-11-19 23:51:31,867 : INFO : KeyedVectors lifecycle event {'fname': 'googleNews.d2v', 'datetime': '2021-11-19T23:51:29.636261', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "try:\n",
    "    model_loaded = gensim.models.keyedvectors.KeyedVectors.load('googleNews.d2v')\n",
    "except:\n",
    "    model_loaded = api.load('word2vec-google-news-300')\n",
    "    model_loaded.save('googleNews.d2v')\n",
    "    model_loaded = gensim.models.keyedvectors.KeyedVectors.load('googleNews.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files\n",
    "filepath=\"data/DBpedia_map_type_entities.json\"\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    DBpedia_map_type_entities = json.load(file)\n",
    "    \n",
    "filepath=\"data/training_types.json\"\n",
    "with open(filepath,encoding='utf-8') as json_file:\n",
    "    training_map_type_questions = json.load(json_file)\n",
    "\n",
    "filepath=\"data/ElasticSearch_map_type_docID.json\"\n",
    "with open(filepath, 'r',encoding='utf-8') as f:\n",
    "    docID_DBOtype_dict = json.load(f)\n",
    "\n",
    "filepath=\"../smart-dataset/datasets/DBpedia/smarttask_dbpedia_train.json\"\n",
    "with open(filepath, 'r') as f:\n",
    "    smarttask_dbpedia_train = json.load(f)\n",
    "\n",
    "typeobj=TypeTaxonomy(\"data/dbpedia_types.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_list=list(typeobj._types.keys())\n",
    "# type_list[1:]\n",
    "# len(type_list[1:]),len(smarttask_dbpedia_train) ,smarttask_dbpedia_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_type=\"dbo:MilitaryPerson\"\n",
    "# question=\"Does the shelf life of spinach equal 8?\"\n",
    "\n",
    "# dp_type=\"dbo:MusicFestival\"\n",
    "# question=\"When was Bibi Andersson music festival married to Per Ahlmark very green?\"\n",
    "\n",
    "# extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "# features_1to5=extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "# extract_features_11to12(dp_type, question,docID_DBOtype_dict,es)\n",
    "# extract_features_13to15(typeobj,dp_type)\n",
    "# print(extract_features_16(DBpedia_map_type_entities,dp_type))\n",
    "\n",
    "# extract_features_17to19(training_map_type_questions,dp_type,question)\n",
    "# extract_features_20to22(dp_type,question)\n",
    "# extract_features_23to25(model_loaded,dp_type, question, mode=\"Euclidean\")\n",
    "# extract_features_23to25(model_loaded,dp_type, question, mode=\"similarities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_type_featureVectors_13to16(typeobj:TypeTaxonomy,\n",
    "                        DBpedia_map_type_entities:Dict\n",
    "                        )->List[Dict[str,List]]:\n",
    "    \n",
    "    map_type_featureVectors={}\n",
    "    all_DBOtype=list(typeobj._types.keys())\n",
    "    #remove the first type:\"thing\"\n",
    "    all_DBOtype=all_DBOtype[1:]\n",
    "    for DBOtype in all_DBOtype:\n",
    "        dict_13to16={}\n",
    "        features_13to15 = extract_features_13to15(typeobj,DBOtype)\n",
    "        feature_vect =list(features_13to15.values())\n",
    "\n",
    "        features_16=extract_features_16(DBpedia_map_type_entities,DBOtype)\n",
    "        feature_vect.extend(list(features_16.values()))\n",
    "     \n",
    "        map_type_featureVectors[DBOtype]=feature_vect\n",
    "        \n",
    "    return map_type_featureVectors\n",
    "        \n",
    "\n",
    "# features_13to16=map_type_featureVectors_13to16(typeobj,DBpedia_map_type_entities)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "    question:str,\n",
    "    dp_type: str,\n",
    "    DBpedia_map_type_entities:Dict,\n",
    "    docID_DBOtype_dict:Dict,\n",
    "    typeobj:TypeTaxonomy,\n",
    "    training_map_type_questions:Dict,\n",
    "    model_loaded:gensim.models.keyedvectors.KeyedVectors,\n",
    "    es: Elasticsearch,\n",
    "    analyze,X,terms_corpus,\n",
    "    features_13to16:List[Dict[str,List]],\n",
    "    add_extra_features:bool=False\n",
    ") -> List[float]:\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Args:\n",
    "            query: string.\n",
    "            dp_type: DBO type.\n",
    "            es: Elasticsearch object instance.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    features_1to5 = extract_features_1to5(DBpedia_map_type_entities,dp_type,question,es)\n",
    "    feature_vect = list(features_1to5.values())\n",
    "\n",
    "    features_11to12 = extract_features_11to12(dp_type, question,docID_DBOtype_dict,es)\n",
    "    feature_vect.extend(list(features_11to12.values()))\n",
    "\n",
    "#     features_13to15 = extract_features_13to15(typeobj,dp_type)\n",
    "#     feature_vect.extend(list(features_13to15.values()))\n",
    "    \n",
    "#     features_16=extract_features_16(DBpedia_map_type_entities,dp_type)\n",
    "#     feature_vect.extend(list(features_16.values()))\n",
    "    feature_vect.extend(features_13to16[dp_type])\n",
    "    \n",
    "   \n",
    "    features_17to19=extract_features_17to19(analyze,X,terms_corpus,training_map_type_questions,dp_type,question)\n",
    "    feature_vect.extend(list(features_17to19.values()))\n",
    "    \n",
    "    features_20to22=extract_features_20to22(dp_type,question)\n",
    "    feature_vect.extend(list(features_20to22.values()))\n",
    "    \n",
    "    features_23to25=extract_features_23to25(model_loaded,dp_type, question, mode=\"Euclidean\")\n",
    "    feature_vect.extend(list(features_23to25.values()))\n",
    "\n",
    "    if add_extra_features:\n",
    "        features_23to25_variant=extract_features_23to25(model_loaded,dp_type, question, mode=\"similarities\")\n",
    "        feature_vect.extend(list(features_23to25_variant.values()))\n",
    "    \n",
    "    return feature_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze,X,terms_corpus=get_analyze(training_map_type_questions)\n",
    "# dp_type=\"dbo:MusicFestival\"\n",
    "# question=\"When was Bibi Andersson music festival married to Per Ahlmark very green?\"\n",
    "# features= extract_features(\n",
    "#                     question,\n",
    "#                     dp_type,\n",
    "#                     DBpedia_map_type_entities,\n",
    "#                     docID_DBOtype_dict,\n",
    "#                     typeobj,\n",
    "#                     training_map_type_questions,\n",
    "#                     model_loaded,\n",
    "#                     es,\n",
    "#                     analyze,X,terms_corpus,\n",
    "#                     features_13to16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(  smarttask_dbpedia_train:List[Dict],\n",
    "                            DBpedia_map_type_entities:Dict[str,List],\n",
    "                            docID_DBOtype_dict:Dict[str,str],\n",
    "                            typeobj:TypeTaxonomy,\n",
    "                            training_map_type_questions:Dict[str,str],\n",
    "                            model_loaded:gensim.models.keyedvectors.KeyedVectors,\n",
    "                            es: Elasticsearch,\n",
    "                            add_extra_features:bool=False)-> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    all_DBOtype=list(typeobj._types.keys())\n",
    "    #remove the first type:\"thing\"\n",
    "    all_DBOtype=all_DBOtype[1:]\n",
    "    analyze,X,terms_corpus=get_analyze(training_map_type_questions)\n",
    "    features_13to16=map_type_featureVectors_13to16(typeobj,DBpedia_map_type_entities)    \n",
    "    count=0\n",
    "    j=0\n",
    "    for entry in smarttask_dbpedia_train:\n",
    "#         if j%int(len(smarttask_dbpedia_train)/100)==0:\n",
    "#             print(f'--------------{j/int(len(smarttask_dbpedia_train)/100)}% is finished')\n",
    "        if j%100==0:\n",
    "            print(\"--------------------j:\",j)\n",
    "        \n",
    "        j+=1\n",
    "        if entry['question']==None:\n",
    "            continue\n",
    "        if entry['category']=='resource':\n",
    "            for DBOtype in all_DBOtype: \n",
    "                try:\n",
    "                    features=extract_features(\n",
    "                    entry['question'],\n",
    "                    DBOtype,\n",
    "                    DBpedia_map_type_entities,\n",
    "                    docID_DBOtype_dict,\n",
    "                    typeobj,\n",
    "                    training_map_type_questions,\n",
    "                    model_loaded,\n",
    "                    es,\n",
    "                    analyze,X,terms_corpus,\n",
    "                    features_13to16) \n",
    "                except BaseException as err:\n",
    "                    print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                    print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                    raise\n",
    "                X_train.append(features)\n",
    "                count+=1\n",
    "                #print(f'--------------length of X_train:{len(X_train)} y_train: {len(y_train)}')\n",
    "                if DBOtype in entry['type']: \n",
    "                    y_train.append(1)\n",
    "                else:\n",
    "                    y_train.append(0)\n",
    "                    \n",
    "                if count%100000==0:\n",
    "                    print(f'--------------{count} is finished')\n",
    "                if len(X_train)%500==0:\n",
    "                    print(\"-----------------\",datetime.datetime.now())\n",
    "                    print(f'--------------length of X_train:{len(X_train)} y_train: {len(y_train)}')\n",
    "        \n",
    "    print(f'{count} features has been saved,{j} questions have been processed')\n",
    "    return X_train,y_train\n",
    "\n",
    "\n",
    "# with open(\"../data/training_types.json\", 'w',encoding='utf-8') as f:\n",
    "#   json.dump(collections, f, ensure_ascii=False)\n",
    "\n",
    "X_train,y_train=prepare_training_data(smarttask_dbpedia_train,\n",
    "                                        DBpedia_map_type_entities,\n",
    "                                        docID_DBOtype_dict,\n",
    "                                        typeobj,\n",
    "                                        training_map_type_questions,\n",
    "                                        model_loaded,\n",
    "                                        es) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/X_train.json\", 'w') as f:\n",
    "  json.dump(collections, f)\n",
    "with open(\"../data/y_train.json\", 'w') as f:\n",
    "  json.dump(collections, f)\n",
    "print(\"X_train,y_train are saved as json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es=Elasticsearch()\n",
    "# es.get(index='dbpdiea_type_centric',id=\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index='dbpdiea_type_centric'\n",
    "# field=\"abstract\"\n",
    "# tv = es.termvectors(index=index, doc_type=\"_doc\", id=\"1\", fields=field, term_statistics=True) \n",
    "# collection_len=tv[\"term_vectors\"][field]['field_statistics']['sum_ttf']  \n",
    "# #total number of entity in the collections\n",
    "# doc_number=tv[\"term_vectors\"][field]['field_statistics']['doc_count'] \n",
    "# #average document length\n",
    "# avgdl=collection_len/doc_number\n",
    "# collection_len,doc_number,avgdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word=\"theatre\"\n",
    "# #model_loaded[word]\n",
    "# model_loaded.most_similar(positive=[word], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
