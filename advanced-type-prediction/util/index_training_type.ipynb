{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index the training dataset into elasticSearch according to their DBOtype**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# %load helper_function.py\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def load_dict_from_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except:\n",
    "        print(f'File \\'{filename}\\' not found.')\n",
    "        return None\n",
    "\n",
    "def save_dict_to_json(doc, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(doc, f)\n",
    "\n",
    "\n",
    "def reset_index(es: Elasticsearch, index_name: str, index_settings) -> None:\n",
    "    \"\"\"Clears index\"\"\"\n",
    "    if es.indices.exists(index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "\n",
    "    es.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "def preprocess(doc: str) -> str:\n",
    "    \"\"\"Preprocesses text to prepare it for feature extraction.\n",
    "\n",
    "    Args:\n",
    "        doc: String comprising the unprocessed contents of some email file.\n",
    "\n",
    "    Returns:\n",
    "        String comprising the corresponding preprocessed text.\n",
    "    \"\"\"\n",
    "    re_html = re.compile(\"<[^>]+>\")\n",
    "    doc = re_html.sub(\" \", doc)\n",
    "    #remove pure digits \n",
    "    doc=re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\",\"\",doc)\n",
    "    # Replace punctuation marks (including hyphens) with spaces.\n",
    "    for c in string.punctuation:\n",
    "        doc = doc.replace(c, \" \")\n",
    "    return doc.lower()\n",
    "\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self,index: str,index_settings:dict, reset=True):\n",
    "        self.dictionary = {}     \n",
    "        self.index = index\n",
    "        self.index_settings= index_settings\n",
    "        es = Elasticsearch()\n",
    "        es.info()\n",
    "        self.es = es\n",
    "        if reset:\n",
    "            self.reset_index()\n",
    "\n",
    "    \n",
    "    def preprocess(self, doc: str) -> str:\n",
    "        \"\"\"Preprocesses text to prepare it for feature extraction.\n",
    "\n",
    "    Args:\n",
    "        doc: String comprising the unprocessed contents of some email file.\n",
    "\n",
    "    Returns:\n",
    "        String comprising the corresponding preprocessed text.\n",
    "    \"\"\"\n",
    "        re_html = re.compile(\"<[^>]+>\")\n",
    "        doc = re_html.sub(\" \", doc)\n",
    "        #remove pure digits \n",
    "        doc=re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\",\"\",doc)\n",
    "        # Replace punctuation marks (including hyphens) with spaces.\n",
    "        for c in string.punctuation:\n",
    "            doc = doc.replace(c, \" \")\n",
    "        return doc.lower()\n",
    "\n",
    "    def reset_index(self) -> None:\n",
    "        \"\"\"Clears index\"\"\"\n",
    "        if self.es.indices.exists(self.index):\n",
    "            self.es.indices.delete(index=self.index)\n",
    "\n",
    "        self.es.indices.create(index=self.index, body=self.index_settings)\n",
    "    \n",
    "    def bulk_index(self,data) -> None:\n",
    "        \"\"\"Indexes documents from JSONL file.\"\"\"\n",
    "        bulk_data = []\n",
    "        for item in data:\n",
    "            bulk_data.append(\n",
    "                {\"index\": {\"_index\": self.index, \"_id\": item.pop(\"id\")}}\n",
    "            )\n",
    "            bulk_data.append(item)\n",
    "        self.es.bulk(index=self.index, body=bulk_data, refresh=True)\n",
    "    \n",
    "    def check_esIndex_count(self)->int:\n",
    "        self.es.indices.refresh(self.index)\n",
    "        count = self.es.cat.count(self.index, params={\"format\": \"json\"})\n",
    "        return int(count[0][\"count\"])\n",
    "\n",
    "    def check_esIndex_content(self, id:str):\n",
    "        return self.es.get(index=self.index, id=id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----i 17571\n",
      "----j 9573\n"
     ]
    }
   ],
   "source": [
    "def prepare_bulk_data(data):\n",
    "    \n",
    "    \"\"\"prepare data  for bulk interface indexing to elasticsearch\n",
    "    \n",
    "        returns: bulked data\n",
    "    \"\"\"\n",
    "    dic={}\n",
    "    i=0\n",
    "    j=0\n",
    "    for entry in data:\n",
    "        i+=1\n",
    "        if entry['question']==None:\n",
    "            continue\n",
    "        if entry['category']=='resource':\n",
    "            j+=1\n",
    "            processed_question=preprocess(entry['question'])\n",
    "            for item in entry['type']:      \n",
    "                #print(processed_question)\n",
    "                dic[item]=dic.get(item,\"\")+processed_question\n",
    "    collections=[{\"id\":str(i),\"type\":DBOtype,\"questions\":dic[DBOtype]} for i,DBOtype in enumerate(dic)]\n",
    "    print(\"----i\",i)\n",
    "    print(\"----j\",j)\n",
    "    return collections\n",
    "\n",
    "training_data=load_dict_from_json(\"../../smart-dataset/datasets/DBpedia/smarttask_dbpedia_train.json\")\n",
    "prepare_bulk_data(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----i 17571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junec\\AppData\\Local\\Temp/ipykernel_20572/1590620374.py:80: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  if self.es.indices.exists(self.index):\n",
      "C:\\Users\\junec\\AppData\\Local\\Temp/ipykernel_20572/1590620374.py:83: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  self.es.indices.create(index=self.index, body=self.index_settings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junec\\AppData\\Local\\Temp/ipykernel_20572/1590620374.py:96: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  self.es.indices.refresh(self.index)\n",
      "C:\\Users\\junec\\AppData\\Local\\Temp/ipykernel_20572/1590620374.py:97: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  count = self.es.cat.count(self.index, params={\"format\": \"json\"})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_index': 'trainning_type_questions',\n",
       " '_type': '_doc',\n",
       " '_id': '300',\n",
       " '_version': 1,\n",
       " '_seq_no': 300,\n",
       " '_primary_term': 1,\n",
       " 'found': True,\n",
       " '_source': {'type': 'dbo:Glacier',\n",
       "  'questions': 'which is  tributary  of  next to lake  of  spitsbergen   '}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_NAME = \"trainning_type_questions\"\n",
    "INDEX_SETTINGS = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"type\": {\n",
    "                \"type\": \"text\",\n",
    "                \"term_vector\": \"yes\",\n",
    "                \"analyzer\": \"english\",\n",
    "            },\n",
    "            \"questions\": {\n",
    "                \"type\": \"text\",\n",
    "                \"term_vector\": \"yes\",\n",
    "                \"analyzer\": \"english\",\n",
    "            },\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n",
    "training_data=load_dict_from_json(\"../../smart-dataset/datasets/DBpedia/smarttask_dbpedia_train.json\")\n",
    "collections=prepare_bulk_data(training_data)\n",
    "index_trainning_type=Indexer(INDEX_NAME,INDEX_SETTINGS)\n",
    "index_trainning_type.bulk_index(collections)\n",
    "len_indexed=index_trainning_type.check_esIndex_count()\n",
    "print(f'{len_indexed} items are indexed in elasticSearch, index name is {INDEX_NAME}')\n",
    "#index_trainning_type.check_esIndex_content(\"300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/training_types.json\", 'w',encoding='utf-8') as f:\n",
    "  json.dump(collections, f, ensure_ascii=False)\n",
    "print(\"training_types.json are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
