{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------preparing training data for type_label features\n"
     ]
    }
   ],
   "source": [
    "print(\"---------preparing training data for type_label features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\junec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys,json,re,string\n",
    "from typing import Callable, Dict, List, Set, Tuple\n",
    "from elasticsearch import Elasticsearch\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import datetime\n",
    "import csv\n",
    "sys.path.insert(1, 'extract_features')\n",
    "sys.path.insert(1, 'util')\n",
    "from helper_function import preprocess\n",
    "from get_alltypes_test_questions import get_alltypes_test_questions\n",
    "from extract_No1to5_features import extract_features_1to5\n",
    "from extract_No11to12_features import extract_features_11to12\n",
    "from extract_No13to15_features import TypeTaxonomy, extract_features_13to15\n",
    "from extract_No16_feature import extract_features_16\n",
    "from extract_No17to19_features import get_analyze,extract_features_17to19\n",
    "from extract_No20to22_features import extract_features_20to22\n",
    "from extract_No23to25_features_optimized import extract_features_23to25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'LAPTOP-ADBLIUPR',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': '5VELmBwJTk-urTuhZdTgew',\n",
       " 'version': {'number': '7.15.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'zip',\n",
       "  'build_hash': '83c34f456ae29d60e94d886e455e6a3409bba9ed',\n",
       "  'build_date': '2021-10-07T21:56:19.031608185Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.9.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce logging level.\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "logging.getLogger(\"elasticsearch\").disabled = True\n",
    "#set a long timeout\n",
    "es= Elasticsearch(timeout=600)\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/junec/anaconda3/python\n",
      "/c/Users/junec/anaconda3/Scripts/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 14:44:36,914 : INFO : loading KeyedVectors object from googleNews.d2v\n",
      "2021-11-21 14:44:38,228 : INFO : loading vectors from googleNews.d2v.vectors.npy with mmap=None\n",
      "2021-11-21 14:44:43,324 : INFO : KeyedVectors lifecycle event {'fname': 'googleNews.d2v', 'datetime': '2021-11-21T14:44:43.318638', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "try:\n",
    "    model_loaded = gensim.models.keyedvectors.KeyedVectors.load('googleNews.d2v')\n",
    "except:\n",
    "    model_loaded = api.load('word2vec-google-news-300')\n",
    "    model_loaded.save('googleNews.d2v')\n",
    "    model_loaded = gensim.models.keyedvectors.KeyedVectors.load('googleNews.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files\n",
    "filepath=\"data/DBpedia_map_type_entities.json\"\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    DBpedia_map_type_entities = json.load(file)\n",
    "    \n",
    "filepath=\"data/training_types.json\"\n",
    "with open(filepath,encoding='utf-8') as json_file:\n",
    "    training_map_type_questions = json.load(json_file)\n",
    "\n",
    "filepath=\"data/ElasticSearch_map_type_docID.json\"\n",
    "with open(filepath, 'r',encoding='utf-8') as f:\n",
    "    docID_DBOtype_dict = json.load(f)\n",
    "\n",
    "filepath=\"../smart-dataset/datasets/DBpedia/smarttask_dbpedia_train.json\"\n",
    "with open(filepath, 'r') as f:\n",
    "    smarttask_dbpedia_train = json.load(f)\n",
    "\n",
    "typeobj=TypeTaxonomy(\"data/dbpedia_types.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_type_label(\n",
    "    question:str,\n",
    "    dp_type: str,\n",
    "    training_map_type_questions:Dict,\n",
    "    model_loaded:gensim.models.keyedvectors.KeyedVectors,\n",
    "    es: Elasticsearch,\n",
    "    analyze,X,terms_corpus,\n",
    "    add_extra_features:bool=False\n",
    ") -> List[float]:\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Args:\n",
    "            query: string.\n",
    "            dp_type: DBO type.\n",
    "            es: Elasticsearch object instance.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"  \n",
    "    features_17to19=extract_features_17to19(analyze,X,terms_corpus,training_map_type_questions,dp_type,question)\n",
    "    feature_vect=list(features_17to19.values())\n",
    "    \n",
    "    features_20to22=extract_features_20to22(dp_type,question)\n",
    "    feature_vect.extend(list(features_20to22.values()))\n",
    "    \n",
    "    features_23to25=extract_features_23to25(model_loaded,dp_type, question, mode=\"Euclidean\")\n",
    "    feature_vect.extend(list(features_23to25.values()))\n",
    "\n",
    "    if add_extra_features:\n",
    "        features_23to25_variant=extract_features_23to25(model_loaded,dp_type, question, mode=\"similarities\")\n",
    "        feature_vect.extend(list(features_23to25_variant.values()))\n",
    "    \n",
    "    return feature_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------j: 0\n",
      "----------------- 2021-11-21 14:44:46.214524\n",
      "--------------------count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junec\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\junec\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- 2021-11-21 14:45:37.845377\n",
      "--------------------count: 30\n",
      "--------------------j: 100\n",
      "----------------- 2021-11-21 14:46:26.668322\n",
      "--------------------count: 60\n",
      "----------------- 2021-11-21 14:47:14.399290\n",
      "--------------------count: 90\n",
      "--------------------j: 200\n",
      "----------------- 2021-11-21 14:47:58.876212\n",
      "--------------------count: 120\n",
      "----------------- 2021-11-21 14:48:54.386983\n",
      "--------------------count: 150\n",
      "--------------------j: 300\n",
      "----------------- 2021-11-21 14:49:42.602836\n",
      "--------------------count: 180\n",
      "----------------- 2021-11-21 14:50:36.363810\n",
      "--------------------count: 210\n",
      "--------------------j: 400\n",
      "----------------- 2021-11-21 14:51:22.247921\n",
      "--------------------count: 240\n",
      "----------------- 2021-11-21 14:52:09.033424\n",
      "--------------------count: 270\n",
      "--------------------j: 500\n",
      "----------------- 2021-11-21 14:53:00.724295\n",
      "--------------------count: 300\n",
      "----------------- 2021-11-21 14:53:45.970080\n",
      "--------------------count: 330\n",
      "--------------------j: 600\n",
      "----------------- 2021-11-21 14:54:32.972165\n",
      "--------------------count: 360\n",
      "----------------- 2021-11-21 14:55:23.794595\n",
      "--------------------count: 390\n",
      "--------------------j: 700\n",
      "----------------- 2021-11-21 14:56:14.291856\n",
      "--------------------count: 420\n",
      "----------------- 2021-11-21 14:57:01.045002\n",
      "--------------------count: 450\n",
      "--------------------j: 800\n",
      "----------------- 2021-11-21 14:57:51.174876\n",
      "--------------------count: 480\n",
      "--------------------j: 900\n",
      "----------------- 2021-11-21 14:58:40.408545\n",
      "--------------------count: 510\n",
      "----------------- 2021-11-21 14:59:30.579523\n",
      "--------------------count: 540\n",
      "--------------------j: 1000\n",
      "----------------- 2021-11-21 15:00:17.186573\n",
      "--------------------count: 570\n",
      "----------------- 2021-11-21 15:01:06.219853\n",
      "--------------------count: 600\n",
      "--------------------j: 1100\n",
      "----------------- 2021-11-21 15:01:52.340598\n",
      "--------------------count: 630\n",
      "----------------- 2021-11-21 15:02:39.811790\n",
      "--------------------count: 660\n",
      "--------------------j: 1200\n",
      "----------------- 2021-11-21 15:03:26.770764\n",
      "--------------------count: 690\n",
      "----------------- 2021-11-21 15:04:11.770251\n",
      "--------------------count: 720\n",
      "--------------------j: 1300\n",
      "----------------- 2021-11-21 15:05:03.195786\n",
      "--------------------count: 750\n",
      "--------------------j: 1400\n",
      "----------------- 2021-11-21 15:05:55.636617\n",
      "--------------------count: 780\n",
      "----------------- 2021-11-21 15:06:42.188544\n",
      "--------------------count: 810\n",
      "----------------- 2021-11-21 15:07:32.948497\n",
      "--------------------count: 840\n",
      "--------------------j: 1500\n",
      "----------------- 2021-11-21 15:08:19.495077\n",
      "--------------------count: 870\n",
      "--------------------j: 1600\n",
      "----------------- 2021-11-21 15:09:07.427532\n",
      "--------------------count: 900\n",
      "----------------- 2021-11-21 15:09:53.165683\n",
      "--------------------count: 930\n",
      "--------------------j: 1700\n",
      "----------------- 2021-11-21 15:10:43.947033\n",
      "--------------------count: 960\n",
      "----------------- 2021-11-21 15:11:31.115329\n",
      "--------------------count: 990\n",
      "--------------------j: 1800\n",
      "----------------- 2021-11-21 15:12:21.444190\n",
      "--------------------count: 1020\n",
      "--------------------j: 1900\n",
      "----------------- 2021-11-21 15:13:09.331601\n",
      "--------------------count: 1050\n",
      "----------------- 2021-11-21 15:13:53.508726\n",
      "--------------------count: 1080\n",
      "--------------------j: 2000\n",
      "----------------- 2021-11-21 15:14:45.390659\n",
      "--------------------count: 1110\n",
      "--------------------j: 2100\n",
      "----------------- 2021-11-21 15:15:33.959105\n",
      "--------------------count: 1140\n",
      "----------------- 2021-11-21 15:16:18.581786\n",
      "--------------------count: 1170\n",
      "--------------------j: 2200\n",
      "----------------- 2021-11-21 15:17:05.403728\n",
      "--------------------count: 1200\n",
      "----------------- 2021-11-21 15:17:55.242405\n",
      "--------------------count: 1230\n",
      "--------------------j: 2300\n",
      "----------------- 2021-11-21 15:18:41.099200\n",
      "--------------------count: 1260\n",
      "--------------------j: 2400\n",
      "----------------- 2021-11-21 15:19:26.353876\n",
      "--------------------count: 1290\n",
      "----------------- 2021-11-21 15:20:12.631453\n",
      "--------------------count: 1320\n",
      "--------------------j: 2500\n",
      "----------------- 2021-11-21 15:21:01.250490\n",
      "--------------------count: 1350\n",
      "----------------- 2021-11-21 15:21:45.821433\n",
      "--------------------count: 1380\n",
      "--------------------j: 2600\n",
      "----------------- 2021-11-21 15:22:32.582315\n",
      "--------------------count: 1410\n",
      "----------------- 2021-11-21 15:23:21.605957\n",
      "--------------------count: 1440\n",
      "--------------------j: 2700\n",
      "----------------- 2021-11-21 15:24:07.042798\n",
      "--------------------count: 1470\n",
      "----------------- 2021-11-21 15:24:55.689251\n",
      "--------------------count: 1500\n",
      "--------------------j: 2800\n",
      "----------------- 2021-11-21 15:25:45.157780\n",
      "--------------------count: 1530\n",
      "----------------- 2021-11-21 15:26:33.604448\n",
      "--------------------count: 1560\n",
      "--------------------j: 2900\n",
      "----------------- 2021-11-21 15:27:20.787637\n",
      "--------------------count: 1590\n",
      "----------------- 2021-11-21 15:28:03.763463\n",
      "--------------------count: 1620\n",
      "--------------------j: 3000\n",
      "----------------- 2021-11-21 15:28:58.620867\n",
      "--------------------count: 1650\n",
      "----------------- 2021-11-21 15:29:46.770347\n",
      "--------------------count: 1680\n",
      "--------------------j: 3100\n",
      "----------------- 2021-11-21 15:30:35.745661\n",
      "--------------------count: 1710\n",
      "----------------- 2021-11-21 15:31:24.560826\n",
      "--------------------count: 1740\n",
      "--------------------j: 3200\n",
      "----------------- 2021-11-21 15:32:13.733637\n",
      "--------------------count: 1770\n",
      "----------------- 2021-11-21 15:33:04.019604\n",
      "--------------------count: 1800\n",
      "--------------------j: 3300\n",
      "----------------- 2021-11-21 15:33:57.523770\n",
      "--------------------count: 1830\n",
      "----------------- 2021-11-21 15:34:43.559058\n",
      "--------------------count: 1860\n",
      "--------------------j: 3400\n",
      "----------------- 2021-11-21 15:35:32.631449\n",
      "--------------------count: 1890\n",
      "--------------------j: 3500\n",
      "----------------- 2021-11-21 15:36:21.160293\n",
      "--------------------count: 1920\n",
      "----------------- 2021-11-21 15:37:07.694946\n",
      "--------------------count: 1950\n",
      "--------------------j: 3600\n",
      "----------------- 2021-11-21 15:37:59.925815\n",
      "--------------------count: 1980\n",
      "----------------- 2021-11-21 15:38:50.679220\n",
      "--------------------count: 2010\n",
      "--------------------j: 3700\n",
      "----------------- 2021-11-21 15:39:42.610647\n",
      "--------------------count: 2040\n",
      "----------------- 2021-11-21 15:40:31.829719\n",
      "--------------------count: 2070\n",
      "--------------------j: 3800\n",
      "----------------- 2021-11-21 15:41:22.787280\n",
      "--------------------count: 2100\n",
      "----------------- 2021-11-21 15:42:15.899679\n",
      "--------------------count: 2130\n",
      "--------------------j: 3900\n",
      "----------------- 2021-11-21 15:43:04.664810\n",
      "--------------------count: 2160\n",
      "--------------------j: 4000\n",
      "----------------- 2021-11-21 15:43:54.410263\n",
      "--------------------count: 2190\n",
      "----------------- 2021-11-21 15:44:52.284148\n",
      "--------------------count: 2220\n",
      "--------------------j: 4100\n",
      "----------------- 2021-11-21 15:45:42.321491\n",
      "--------------------count: 2250\n",
      "--------------------j: 4200\n",
      "----------------- 2021-11-21 15:46:33.092294\n",
      "--------------------count: 2280\n",
      "----------------- 2021-11-21 15:47:25.509902\n",
      "--------------------count: 2310\n",
      "--------------------j: 4300\n",
      "----------------- 2021-11-21 15:48:15.328157\n",
      "--------------------count: 2340\n",
      "----------------- 2021-11-21 15:49:06.387060\n",
      "--------------------count: 2370\n",
      "--------------------j: 4400\n",
      "----------------- 2021-11-21 15:49:54.283830\n",
      "--------------------count: 2400\n",
      "----------------- 2021-11-21 15:50:42.652773\n",
      "--------------------count: 2430\n",
      "--------------------j: 4500\n",
      "----------------- 2021-11-21 15:51:29.546716\n",
      "--------------------count: 2460\n",
      "----------------- 2021-11-21 15:52:16.278077\n",
      "--------------------count: 2490\n",
      "--------------------j: 4600\n",
      "----------------- 2021-11-21 15:53:05.704146\n",
      "--------------------count: 2520\n",
      "----------------- 2021-11-21 15:53:50.969956\n",
      "--------------------count: 2550\n",
      "--------------------j: 4700\n",
      "----------------- 2021-11-21 15:54:41.566797\n",
      "--------------------count: 2580\n",
      "--------------------j: 4800\n",
      "----------------- 2021-11-21 15:55:29.191347\n",
      "--------------------count: 2610\n",
      "----------------- 2021-11-21 15:56:17.743949\n",
      "--------------------count: 2640\n",
      "--------------------j: 4900\n",
      "----------------- 2021-11-21 15:57:13.233907\n",
      "--------------------count: 2670\n",
      "----------------- 2021-11-21 15:58:01.671996\n",
      "--------------------count: 2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------j: 5000\n",
      "----------------- 2021-11-21 15:58:50.432364\n",
      "--------------------count: 2730\n",
      "----------------- 2021-11-21 15:59:36.403685\n",
      "--------------------count: 2760\n",
      "--------------------j: 5100\n",
      "----------------- 2021-11-21 16:00:24.411371\n",
      "--------------------count: 2790\n",
      "----------------- 2021-11-21 16:01:11.329693\n",
      "--------------------count: 2820\n",
      "--------------------j: 5200\n",
      "----------------- 2021-11-21 16:02:02.128706\n",
      "--------------------count: 2850\n",
      "--------------------j: 5300\n",
      "----------------- 2021-11-21 16:02:50.886163\n",
      "--------------------count: 2880\n",
      "----------------- 2021-11-21 16:03:41.468186\n",
      "--------------------count: 2910\n",
      "--------------------j: 5400\n",
      "----------------- 2021-11-21 16:04:29.608055\n",
      "--------------------count: 2940\n",
      "----------------- 2021-11-21 16:05:17.185623\n",
      "--------------------count: 2970\n",
      "--------------------j: 5500\n",
      "----------------- 2021-11-21 16:06:09.779860\n",
      "--------------------count: 3000\n",
      "--------------------j: 5600\n",
      "----------------- 2021-11-21 16:06:56.732933\n",
      "--------------------count: 3030\n",
      "----------------- 2021-11-21 16:07:53.927292\n",
      "--------------------count: 3060\n",
      "--------------------j: 5700\n",
      "----------------- 2021-11-21 16:08:53.882859\n",
      "--------------------count: 3090\n",
      "----------------- 2021-11-21 16:09:51.007072\n",
      "--------------------count: 3120\n",
      "--------------------j: 5800\n",
      "----------------- 2021-11-21 16:10:50.151780\n",
      "--------------------count: 3150\n",
      "----------------- 2021-11-21 16:11:49.585523\n",
      "--------------------count: 3180\n",
      "--------------------j: 5900\n",
      "----------------- 2021-11-21 16:12:48.798221\n",
      "--------------------count: 3210\n",
      "----------------- 2021-11-21 16:13:53.175270\n",
      "--------------------count: 3240\n",
      "--------------------j: 6000\n",
      "----------------- 2021-11-21 16:14:48.748801\n",
      "--------------------count: 3270\n",
      "----------------- 2021-11-21 16:15:43.264574\n",
      "--------------------count: 3300\n",
      "--------------------j: 6100\n",
      "----------------- 2021-11-21 16:16:45.687166\n",
      "--------------------count: 3330\n",
      "----------------- 2021-11-21 16:17:44.369725\n",
      "--------------------count: 3360\n",
      "--------------------j: 6200\n",
      "----------------- 2021-11-21 16:18:47.803586\n",
      "--------------------count: 3390\n",
      "--------------------j: 6300\n",
      "----------------- 2021-11-21 16:19:53.043877\n",
      "--------------------count: 3420\n"
     ]
    }
   ],
   "source": [
    "def prepare_training_data_type_label(  smarttask_dbpedia_train:List[Dict],\n",
    "                                        training_map_type_questions:Dict[str,str],\n",
    "                                        model_loaded:gensim.models.keyedvectors.KeyedVectors,\n",
    "                                        #typeobj:TypeTaxonomy,\n",
    "                                        types_for_training:List[str],\n",
    "                                        add_extra_features:bool=False)-> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"X and y label for type label features. Save to csv file.\n",
    "    \n",
    "         Args:\n",
    "            smarttask_dbpedia_train: smarttask training set.\n",
    "            training_map_type_questions:dictionary for DBOtype and all questions have that type in training dataset.\n",
    "            model_loaded:pretrained gensim model.\n",
    "            add_extra_features:if is true, will add an extra feature to X.\n",
    "            es: Elasticsearch object instance.\n",
    "    \"\"\"\n",
    "\n",
    "    analyze,X,terms_corpus=get_analyze(training_map_type_questions)\n",
    "#     all_DBOtype=list(typeobj._types.keys())[1:]\n",
    "    \n",
    "    count=0\n",
    "    j=0\n",
    "\n",
    "    with open(\"data/for_training_type_label_alltypes.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "\n",
    "        for entry in smarttask_dbpedia_train:\n",
    "            if j%100==0:\n",
    "                print(\"--------------------j:\",j)\n",
    "\n",
    "            j+=1\n",
    "            if entry['question']==None:\n",
    "                continue\n",
    "\n",
    "            question_processed=preprocess(entry['question'])\n",
    "\n",
    "            if entry['category']=='resource':\n",
    "                if count%30==0:\n",
    "                    print(\"-----------------\",datetime.datetime.now())\n",
    "                    print(\"--------------------count:\",count)  \n",
    "#                   print(f'--------------length of X_train:{len(X_train)} y_train: {len(y_train)}')\n",
    "                count+=1\n",
    "                #print(\"question_processed:\",question_processed)\n",
    "                for DBOtype in entry['type']: \n",
    "                    try:\n",
    "                        features=extract_features_type_label(question_processed,\n",
    "                                                            DBOtype,\n",
    "                                                            training_map_type_questions,\n",
    "                                                            model_loaded,\n",
    "                                                            es,\n",
    "                                                            analyze,X,terms_corpus) \n",
    "                    except BaseException as err:\n",
    "                        print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                        print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                        raise\n",
    "\n",
    "                    writer.writerow(features)\n",
    "                    writer.writerow([1])\n",
    "\n",
    "                #deal with top 30 documents\n",
    "#                 hits = es.search(\n",
    "#                     index=\"dbpdiea_type_centric\", q=question_processed, _source=True, size=30\n",
    "#                 )[\"hits\"][\"hits\"]\n",
    "#                 rank_list= [hit['_source'][\"type\"] for hit in hits]\n",
    "#                 #print(rank_list)\n",
    "\n",
    "                for DBOtype in types_for_training:\n",
    "                    if DBOtype not in entry['type']:\n",
    "                        try:\n",
    "                            features=extract_features_type_label(question_processed,\n",
    "                                                            DBOtype,\n",
    "                                                            training_map_type_questions,\n",
    "                                                            model_loaded,\n",
    "                                                            es,\n",
    "                                                            analyze,X,terms_corpus) \n",
    "                        except BaseException as err:\n",
    "                            print(\"------------error for type:\",DBOtype,entry['question'])\n",
    "                            print(f\"Unexpected {err}, {type(err)}\")  \n",
    "                            raise\n",
    "                            \n",
    "                        writer.writerow(features)\n",
    "                        writer.writerow([0])\n",
    "\n",
    "    print(f'{count} 9573 questions with the resource category are processed,{j} questions in total')\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "#just get features for types which exsits in testing question to reduce the time \n",
    "types_for_training=get_alltypes_test_questions(filename=\"../smart-dataset/datasets/DBpedia/smarttask_dbpedia_test.json\")\n",
    "prepare_training_data_type_label(smarttask_dbpedia_train,\n",
    "                                        training_map_type_questions,\n",
    "                                        model_loaded,\n",
    "                                        types_for_training\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
